{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24f0115e-d5a1-4908-9e21-6d524486d75d",
   "metadata": {},
   "source": [
    "## Week 1: Foundations of LLMs and Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e32e7a3-1ba1-4db6-9d94-268b6e989d5f",
   "metadata": {},
   "source": [
    "1. What is an LLM?\n",
    "At its core, a Large Language Model (LLM) is a sophisticated neural network trained on a massive amount of text data. Think of it as an incredibly powerful \"autocomplete\" system. It learns grammar, facts, reasoning abilities, and different styles of writing by predicting the next word in a sentence, over and over again, on a dataset the size of a large portion of the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50009665-fd32-484a-9451-d3498abffb87",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character 'â€™' (U+2019) (1158215849.py, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mExamples: OpenAIâ€™s GPT-4, Googleâ€™s Gemini, Metaâ€™s LLaMA.\u001b[39m\n                    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid character 'â€™' (U+2019)\n"
     ]
    }
   ],
   "source": [
    "Definition: A type of artificial intelligence model trained on massive amounts of text data to understand and generate human-like language.\n",
    "\n",
    "Examples: OpenAIâ€™s GPT-4, Googleâ€™s Gemini, Metaâ€™s LLaMA.\n",
    "\n",
    "Uses: Chatbots, text summarization, translation, content generation, code assistance, etc.\n",
    "\n",
    "Key Features:\n",
    "\n",
    "Learns patterns in language.\n",
    "\n",
    "Can perform tasks with little or no fine-tuning (zero-shot or few-shot learning).\n",
    "\n",
    "Often built using transformer architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4826932a-cc1b-483c-bb6a-d438bf3fc987",
   "metadata": {},
   "source": [
    "## The Key Players in LLM Development\n",
    "\n",
    "### ðŸ”· GPT (Generative Pre-trained Transformer) Series by OpenAI\n",
    "- **Models**: `GPT-3`, `GPT-3.5`, `GPT-4`\n",
    "- **Strengths**:\n",
    "  - Strong reasoning and instruction following\n",
    "  - Creative text generation\n",
    "- **Access**:\n",
    "  - Primarily via a **paid API**\n",
    " \n",
    "---\n",
    "\n",
    "### ðŸŸ¡ Claude Series by Anthropic\n",
    "- **Models**: `Claude`, `Claude 2`, `Claude 3` *(Haiku, Sonnet, Opus)*\n",
    "- **Strengths**:\n",
    "  - Focus on **AI safety** and *\"Constitutional AI\"*\n",
    "  - Very large **context window** (can process large amounts of text)\n",
    "- **Access**:\n",
    "  - Available through **API**\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¶ LLaMA (Large Language Model Meta AI) by Meta\n",
    "- **Models**: `LLaMA`, `LLaMA 2`, `LLaMA 3`\n",
    "- **Strengths**:\n",
    "  - Powerful **open-source** models\n",
    "  - Can be run locally (with sufficient computing resources)\n",
    "- **Impact**:\n",
    "  - Spurred a wave of **community-driven innovation**\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ“Œ *Note: Open-source models like LLaMA are especially important in academic and research settings due to their flexibility and transparency.*\n",
    "\n",
    "----\n",
    "### DeepSeek-V3 Excels (Best Use Cases)\n",
    "* ðŸ”¹ Long Document Analysis (Research papers, legal docs)\n",
    "* ðŸ”¹ Coding & Debugging (Similar to GPT-4 level)\n",
    "* ðŸ”¹ Free Alternative to GPT-4/Claude Pro\n",
    "* ðŸ”¹ Web-Augmented Answers (When search is enabled)\n",
    "* ðŸ”¹ File Processing (Extracting text/data from PDFs, Excel, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a751ab84-228a-4092-910e-06d592c59c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-2.21.0-py3-none-any.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 5.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.12.0)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.10.0\n",
      "  Downloading jiter-0.13.0-cp311-cp311-win_amd64.whl (204 kB)\n",
      "     ------------------------------------- 204.6/204.6 kB 12.1 MB/s eta 0:00:00\n",
      "Collecting pydantic<3,>=1.9.0\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "     -------------------------------------- 463.6/463.6 kB 9.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sniffio in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.41.5\n",
      "  Downloading pydantic_core-2.41.5-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 12.7 MB/s eta 0:00:00\n",
      "Collecting typing-inspection>=0.4.2\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Installing collected packages: typing-inspection, pydantic-core, jiter, distro, annotated-types, pydantic, openai\n",
      "Successfully installed annotated-types-0.7.0 distro-1.9.0 jiter-0.13.0 openai-2.21.0 pydantic-2.12.5 pydantic-core-2.41.5 typing-inspection-0.4.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install the OpenAI library\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee30ac0d-60fd-404f-a012-f1bb4966e6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- My Prompt ---\n",
      "Explain what a Large Language Model is in three simple sentences.\n",
      "\n",
      "--- GPT's Response ---\n",
      "An error occurred: name 'system_prompt' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Setup your API Key\n",
    "import openai\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"GFGAFGAGSDFGSDAGDAFGAFGAFDGADFGAGSDGADG\"\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "def ask_gpt(prompt_text,text):\n",
    "    \"\"\"\n",
    "    A simple function to send a prompt to the OpenAI API\n",
    "    and get a response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai.OpenAI()\n",
    "        system_promt = \"\"\"You are a helpful assistant.\n",
    "       \n",
    "       \n",
    "        context:{text}\n",
    "\n",
    "        answer this query:\n",
    "        {prompt_text}\n",
    "        \"\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt },\n",
    "                {\"role\": \"user\", \"content\": prompt_text}\n",
    "            ]\n",
    "        )\n",
    "        print(\"Response:\", response)\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "# Step 4: Ask our first question!\n",
    "context = \"\"\n",
    "prompt = \"Explain what a Large Language Model is in three simple sentences.\"\n",
    "response_text = ask_gpt(prompt, \n",
    "\n",
    "context)\n",
    "\n",
    "print(\"--- My Prompt ---\")\n",
    "print(prompt)\n",
    "print(\"\\n--- GPT's Response ---\")\n",
    "print(response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e70594a9-d528-478b-ae80-e12ad51ea459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-generativeai\n",
      "  Downloading google_generativeai-0.8.6-py3-none-any.whl (155 kB)\n",
      "     -------------------------------------- 155.1/155.1 kB 1.3 MB/s eta 0:00:00\n",
      "Collecting google-ai-generativelanguage==0.6.15\n",
      "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "     ---------------------------------------- 1.3/1.3 MB 6.0 MB/s eta 0:00:00\n",
      "Collecting google-api-core\n",
      "  Downloading google_api_core-2.29.0-py3-none-any.whl (173 kB)\n",
      "     -------------------------------------- 173.9/173.9 kB 3.5 MB/s eta 0:00:00\n",
      "Collecting google-api-python-client\n",
      "  Downloading google_api_python_client-2.190.0-py3-none-any.whl (14.7 MB)\n",
      "     --------------------------------------- 14.7/14.7 MB 16.4 MB/s eta 0:00:00\n",
      "Collecting google-auth>=2.15.0\n",
      "  Downloading google_auth-2.48.0-py3-none-any.whl (236 kB)\n",
      "     ------------------------------------- 236.5/236.5 kB 14.1 MB/s eta 0:00:00\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-6.33.5-cp310-abi3-win_amd64.whl (437 kB)\n",
      "     ------------------------------------- 437.1/437.1 kB 28.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pydantic in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-generativeai) (2.12.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-generativeai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-generativeai) (4.15.0)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3\n",
      "  Downloading proto_plus-1.27.1-py3-none-any.whl (50 kB)\n",
      "     ---------------------------------------- 50.5/50.5 kB 2.5 MB/s eta 0:00:00\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-5.29.6-cp310-abi3-win_amd64.whl (435 kB)\n",
      "     ------------------------------------- 435.2/435.2 kB 26.6 MB/s eta 0:00:00\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "     ------------------------------------- 181.3/181.3 kB 11.4 MB/s eta 0:00:00\n",
      "Collecting cryptography>=38.0.3\n",
      "  Downloading cryptography-46.0.5-cp311-abi3-win_amd64.whl (3.5 MB)\n",
      "     ---------------------------------------- 3.5/3.5 MB 15.8 MB/s eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.72.0-py3-none-any.whl (297 kB)\n",
      "     -------------------------------------- 297.5/297.5 kB 9.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.5)\n",
      "Collecting httplib2<1.0.0,>=0.19.0\n",
      "  Downloading httplib2-0.31.2-py3-none-any.whl (91 kB)\n",
      "     ---------------------------------------- 91.1/91.1 kB 5.1 MB/s eta 0:00:00\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0\n",
      "  Downloading google_auth_httplib2-0.3.0-py3-none-any.whl (9.5 kB)\n",
      "Collecting uritemplate<5,>=3.0.1\n",
      "  Downloading uritemplate-4.2.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic->google-generativeai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cryptography>=38.0.3->google-auth>=2.15.0->google-generativeai) (2.0.0)\n",
      "Collecting grpcio<2.0.0,>=1.33.2\n",
      "  Downloading grpcio-1.78.0-cp311-cp311-win_amd64.whl (4.8 MB)\n",
      "     ---------------------------------------- 4.8/4.8 MB 11.8 MB/s eta 0:00:00\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2\n",
      "  Downloading grpcio_status-1.78.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pyparsing<4,>=3.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.3.2)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1\n",
      "  Downloading pyasn1-0.6.2-py3-none-any.whl (83 kB)\n",
      "     ---------------------------------------- 83.4/83.4 kB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2026.1.4)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=2.0.0->cryptography>=38.0.3->google-auth>=2.15.0->google-generativeai) (2.23)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2\n",
      "  Downloading grpcio_status-1.76.0-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.75.1-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.75.0-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.74.0-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.73.1-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.73.0-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.72.2-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.72.1-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.71.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: uritemplate, pyasn1, protobuf, httplib2, grpcio, rsa, pyasn1-modules, proto-plus, googleapis-common-protos, cryptography, grpcio-status, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "Successfully installed cryptography-46.0.5 google-ai-generativelanguage-0.6.15 google-api-core-2.29.0 google-api-python-client-2.190.0 google-auth-2.48.0 google-auth-httplib2-0.3.0 google-generativeai-0.8.6 googleapis-common-protos-1.72.0 grpcio-1.78.0 grpcio-status-1.71.2 httplib2-0.31.2 proto-plus-1.27.1 protobuf-5.29.6 pyasn1-0.6.2 pyasn1-modules-0.4.2 rsa-4.9.1 uritemplate-4.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install the Google Generative AI library\n",
    "!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b57e26a-90ff-46ef-ae27-85b84a8fc7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_20060\\1219050549.py:2: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  import google.generativeai as genai\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask the Query: Who will be the next PM of nepal?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- My Prompt ---\n",
      "Who will be the next PM of nepal?\n",
      "\n",
      "--- Gemini's Response ---\n",
      "Predicting the \"next\" Prime Minister of Nepal is challenging because Nepal's political landscape is characterized by **coalition governments** that are often fluid and can change.\n",
      "\n",
      "**Currently, Pushpa Kamal Dahal 'Prachanda' of the CPN (Maoist Centre) is the Prime Minister.** He leads a multi-party coalition.\n",
      "\n",
      "**Who the \"next\" PM will be depends entirely on future political developments, such as:**\n",
      "\n",
      "1.  **The stability of the current coalition:** If the current coalition remains stable until the next general election, Prachanda could continue. However, Nepali coalitions are known for realigning.\n",
      "2.  **The outcome of the next general election:** This is the most definitive way for a new PM to emerge. The leader of the largest party or the leader capable of forming a majority coalition after the election would become PM.\n",
      "3.  **A collapse of the current coalition:** If the current coalition breaks down, parties would likely try to form a new government, and a different leader (potentially from the Nepali Congress or CPN-UML) could emerge.\n",
      "4.  **Internal party dynamics:** Leadership changes within the major parties (Nepali Congress, CPN-UML, CPN-Maoist Centre) could also influence who becomes PM.\n",
      "\n",
      "**The most prominent leaders who are historically in contention or could be in the future include:**\n",
      "\n",
      "*   **Sher Bahadur Deuba (Nepali Congress):** Leader of the largest party in the current parliament, a five-time former PM.\n",
      "*   **KP Sharma Oli (CPN-UML):** Leader of the second-largest party, a former PM, highly influential.\n",
      "*   **Pushpa Kamal Dahal 'Prachanda' (CPN-Maoist Centre):** The current PM, also a former PM, and a key player.\n",
      "*   **Rabi Lamichhane (Rastriya Swatantra Party - RSP):** Leader of a significant emerging party, while less likely to lead a majority government directly in the near future, his party could play a kingmaker role.\n",
      "\n",
      "**In summary, there is no way to definitively say who the \"next\" PM will be without knowing how the political landscape will evolve in the coming months or years.** It will depend on coalition agreements, election results, and political maneuvering.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Setup your API Key\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# Set your API key (get it from: https://aistudio.google.com/app/apikey)\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyB6LmTK32elOGT70XrKX5_bAorQe5kncaE\"  \n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# Step 3: Define our interaction function\n",
    "def ask_gemini(prompt_text):\n",
    "   \n",
    "    try:\n",
    "        # Initialize the model (gemini-pro is the text model)\n",
    "        model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "       \n",
    "        # Generate the response\n",
    "        response = model.generate_content(prompt_text)\n",
    "       \n",
    "       \n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "# Step 4: Ask our question!\n",
    "prompt = input(\"Ask the Query:\")\n",
    "response_text = ask_gemini(prompt)\n",
    "\n",
    "print(\"--- My Prompt ---\")\n",
    "print(prompt)\n",
    "print(\"\\n--- Gemini's Response ---\")\n",
    "print(response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4315457b-105b-4b55-abdf-2d5bf1a29f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- My Prompt ---\n",
      "Explain what a Large Language Model is in three simple sentences.\n",
      "\n",
      "--- DeepSeek's Response ---\n",
      "An error occurred: Error code: 401 - {'error': {'message': 'Authentication Fails, Your api key: ****here is invalid', 'type': 'authentication_error', 'param': None, 'code': 'invalid_request_error'}}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import libraries\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Step 2: Set your DeepSeek API Key\n",
    "os.environ[\"DEEPSEEK_API_KEY\"] = \"your_deepseek_api_key_here\"\n",
    "\n",
    "# Step 3: Create client (note the base_url for DeepSeek)\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "def ask_deepseek(prompt_text):\n",
    "    \"\"\"\n",
    "    Sends a prompt to DeepSeek API and returns the response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",  # DeepSeek chat model\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt_text}\n",
    "            ]\n",
    "        )\n",
    "       \n",
    "        return response.choices[0].message.content.strip()\n",
    "   \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "\n",
    "# Step 4: Ask your first question\n",
    "prompt = \"Explain what a Large Language Model is in three simple sentences.\"\n",
    "response_text = ask_deepseek(prompt)\n",
    "\n",
    "print(\"--- My Prompt ---\")\n",
    "print(prompt)\n",
    "print(\"\\n--- DeepSeek's Response ---\")\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fe58b04-cbf8-4567-a6a8-8f836067dcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- My Prompt ---\n",
      "Explain what a Large Language Model is in three simple sentences.\n",
      "\n",
      "--- Grok's Response ---\n",
      "An error occurred: Error code: 400 - {'code': 'Client specified an invalid argument', 'error': 'Model not found: grok-2-latest'}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import libraries\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Step 2: Set your xAI (Grok) API Key\n",
    "os.environ[\"XAI_API_KEY\"] = \"your_xai_api_key_here\"\n",
    "\n",
    "# Step 3: Create client (note the base_url for xAI)\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"XAI_API_KEY\"),\n",
    "    base_url=\"https://api.x.ai/v1\"\n",
    ")\n",
    "\n",
    "def ask_grok(prompt_text):\n",
    "    \"\"\"\n",
    "    Sends a prompt to Grok (xAI) and returns the response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"grok-2-latest\",  # You can change model version if needed\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt_text}\n",
    "            ]\n",
    "        )\n",
    "       \n",
    "        return response.choices[0].message.content.strip()\n",
    "   \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "\n",
    "# Step 4: Ask your first question\n",
    "prompt = \"Explain what a Large Language Model is in three simple sentences.\"\n",
    "response_text = ask_grok(prompt)\n",
    "\n",
    "print(\"--- My Prompt ---\")\n",
    "print(prompt)\n",
    "print(\"\\n--- Grok's Response ---\")\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72c49b58-2f8e-4db9-a979-f7f667072cc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m      6\u001b[39m load_dotenv()\n\u001b[32m      8\u001b[39m api_key = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mHUGGINGFACE_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "   \n",
    "    \"inputs\": \"Explain what a Large Language Model is in three simple sentences.\"\n",
    "}\n",
    "\n",
    "response = requests.post(API_URL, headers=headers, json=payload)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19ee8b09-c701-4409-bd86-aa4a2db5b454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def asks_gemini(prompt_text):\n",
    "    try:\n",
    "        model = genai.GenerativeModel('models/gemini-2.5-flash')\n",
    "       \n",
    "        # Simulate system + user messages in a single prompt\n",
    "        full_prompt = f\"\"\"\n",
    "        [System Instruction: You are a helpful assistant of Canada Project, use the formal language,\n",
    "        don't be rude. your response must be in json format with\n",
    "        \"query\": \"{prompt_text}\"  ,\n",
    "        \"response\":  \"\" ,\n",
    "        \"created_time\": \"\",\n",
    "        Response always in Nepali Language.\n",
    "       \n",
    "        ]\n",
    "        [User Question: {prompt_text}]\n",
    "        \"\"\"\n",
    "       \n",
    "        response = model.generate_content(full_prompt)\n",
    "        ai_response = response.text.strip()\n",
    "        refined_response = re.sub(r\"(^```json\\n|```$)\", \"\", ai_response).strip()\n",
    "        return refined_response\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6440f605-dd95-41ab-b68c-9bf542933fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
